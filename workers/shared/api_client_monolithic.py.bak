"""
Internal API Client for Lightweight Workers

Provides HTTP client for communicating with Django backend internal APIs.
Focuses on database operations and basic API coordination only.
WorkflowExecutionService is used directly for workflow execution logic.

Environment Variables for API Endpoint Configuration:
- INTERNAL_API_HEALTH_PREFIX: Override health endpoint prefix (default: 'v1/health/')
- INTERNAL_API_WORKFLOW_PREFIX: Override workflow execution prefix (default: 'api/v1/workflow-execution/')
- INTERNAL_API_ORGANIZATION_PREFIX: Override organization prefix (default: 'api/v1/organization/')
"""

import logging
import json
import os
import time
import uuid
from uuid import UUID
from typing import Dict, Any, Optional, Union, List
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from .config import WorkerConfig
from .logging_utils import WorkerLogger
from .retry_utils import circuit_breaker, CircuitBreakerOpenError

# Import shared dataclasses for type safety and consistency
from unstract.core.data_models import (
    WorkflowFileExecutionData,
    FileHashData,
    FileExecutionCreateRequest,
    FileExecutionStatusUpdateRequest
)

logger = WorkerLogger.get_logger(__name__)


class InternalAPIClientError(Exception):
    """Base exception for API client errors."""
    pass


class AuthenticationError(InternalAPIClientError):
    """Raised when API authentication fails."""
    pass


class APIRequestError(InternalAPIClientError):
    """Raised when API request fails."""
    pass


class InternalAPIClient:
    """
    HTTP client for communicating with Django backend internal APIs.

    Features:
    - Bearer token authentication
    - Automatic retries with exponential backoff
    - Request/response logging
    - Organization context support
    - Circuit breaker pattern
    - Standard CRUD operations
    """

    # Internal API URL patterns - simplified for DB operations only
    # Can be overridden via environment variables for flexibility
    API_ENDPOINTS = {
        'health': os.getenv('INTERNAL_API_HEALTH_PREFIX', 'v1/health/'),
        'workflow_execution': os.getenv('INTERNAL_API_WORKFLOW_PREFIX', 'api/v1/workflow-execution/'),
        'organization': os.getenv('INTERNAL_API_ORGANIZATION_PREFIX', 'api/v1/organization/'),
        'execution': os.getenv('INTERNAL_API_EXECUTION_PREFIX', 'v1/execution/'),
        'tool_execution': os.getenv('INTERNAL_API_TOOL_EXECUTION_PREFIX', 'v1/tool-execution/'),
        'file_execution': os.getenv('INTERNAL_API_FILE_EXECUTION_PREFIX', 'v1/file-execution/'),
        'file_history': os.getenv('INTERNAL_API_FILE_HISTORY_PREFIX', 'v1/file-history/'),
        'webhook': os.getenv('INTERNAL_API_WEBHOOK_PREFIX', 'v1/webhook/'),
    }

    def __init__(self, config: Optional[WorkerConfig] = None):
        """
        Initialize API client.

        Args:
            config: Worker configuration. If None, uses default config.
        """
        self.config = config or WorkerConfig()
        self.base_url = self.config.internal_api_base_url
        self.api_key = self.config.internal_api_key
        self.organization_id = self.config.organization_id

        # Initialize requests session with retry strategy
        self.session = requests.Session()
        self._setup_session()

        logger.info(f"Initialized InternalAPIClient for {self.base_url}")
        logger.debug(f"API endpoint configuration: {self.get_endpoint_config()}")

    def get_endpoint_config(self) -> Dict[str, str]:
        """Get current API endpoint configuration for debugging."""
        return dict(self.API_ENDPOINTS)

    def _build_url(self, endpoint_key: str, path: str = '') -> str:
        """
        Build consistent API URL using endpoint patterns.

        Args:
            endpoint_key: Key from API_ENDPOINTS dict
            path: Additional path to append

        Returns:
            Complete endpoint path
        """
        base_path = self.API_ENDPOINTS.get(endpoint_key, endpoint_key)
        if path:
            return f"{base_path.rstrip('/')}/{path.lstrip('/')}"
        # Preserve trailing slashes in base_path to avoid 301 redirects
        return base_path

    def _setup_session(self):
        """Configure session with retry strategy and timeouts."""
        # Retry strategy
        retry_strategy = Retry(
            total=self.config.api_retry_attempts,
            backoff_factor=self.config.api_retry_backoff_factor,
            status_forcelist=[429, 500, 502, 503, 504],
            allowed_methods=["HEAD", "GET", "POST", "PUT", "DELETE", "OPTIONS", "TRACE"]
        )

        # HTTP adapter with retry
        adapter = HTTPAdapter(max_retries=retry_strategy)
        self.session.mount("http://", adapter)
        self.session.mount("https://", adapter)

        # Default headers
        self.session.headers.update({
            'Authorization': f'Bearer {self.api_key}',
            'Content-Type': 'application/json',
            'User-Agent': f'UnstractWorker/{self.config.worker_version}'
        })

        # Organization context header
        if self.organization_id:
            self.session.headers['X-Organization-ID'] = self.organization_id

    def _serialize_data(self, data: Any) -> Any:
        """
        Recursively serialize data to JSON-compatible format.
        Converts UUID objects, datetime objects, and other complex types to strings.
        """
        import datetime

        if isinstance(data, uuid.UUID):
            return str(data)
        elif isinstance(data, (datetime.datetime, datetime.date)):
            # Convert datetime objects to ISO format strings
            if isinstance(data, datetime.datetime):
                return data.isoformat()
            else:
                return data.isoformat()
        elif isinstance(data, datetime.time):
            # Handle time objects
            return data.isoformat()
        elif isinstance(data, dict):
            return {key: self._serialize_data(value) for key, value in data.items()}
        elif isinstance(data, list):
            return [self._serialize_data(item) for item in data]
        elif isinstance(data, tuple):
            # Handle tuples by converting to list
            return [self._serialize_data(item) for item in data]
        elif isinstance(data, set):
            # Handle sets by converting to list
            return [self._serialize_data(item) for item in data]
        else:
            return data

    def _make_request(
        self,
        method: str,
        endpoint: str,
        data: Optional[Dict[str, Any]] = None,
        params: Optional[Dict[str, Any]] = None,
        timeout: Optional[int] = None,
        max_retries: int = 3,
        backoff_factor: float = 0.5,
        organization_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Enhanced HTTP request with robust error handling and retry logic.

        Args:
            method: HTTP method (GET, POST, PUT, DELETE)
            endpoint: API endpoint (relative to base URL)
            data: Request payload for POST/PUT
            params: Query parameters
            timeout: Request timeout in seconds
            max_retries: Maximum number of retry attempts (default: 3)
            backoff_factor: Exponential backoff factor (default: 0.5)

        Returns:
            Response data as dictionary

        Raises:
            AuthenticationError: If authentication fails
            APIRequestError: If request fails after all retries
        """
        url = f"{self.base_url.rstrip('/')}/{endpoint.lstrip('/')}"
        timeout = timeout or self.config.api_timeout

        last_exception = None
        retry_statuses = {500, 502, 503, 504}  # Server errors to retry

        for attempt in range(max_retries + 1):
            try:
                is_retry = attempt > 0
                log_level = logging.WARNING if is_retry else logging.DEBUG
                logger.log(log_level, f"Making {method} request to {url} (attempt {attempt + 1}/{max_retries + 1})")

                # Prepare request data with enhanced serialization
                kwargs = {
                    'timeout': timeout,
                    'params': params,
                    'allow_redirects': True
                }

                # Handle dynamic organization context
                headers = {}
                current_org_id = organization_id or self.organization_id
                if current_org_id:
                    headers['X-Organization-ID'] = current_org_id
                    if attempt == 0:  # Only log on first attempt
                        logger.debug(f"Including organization header: X-Organization-ID={current_org_id}")

                if headers:
                    kwargs['headers'] = headers

                if data is not None:
                    try:
                        kwargs['json'] = self._serialize_data(data)
                    except Exception as e:
                        logger.error(f"Failed to serialize request data: {e}")
                        raise APIRequestError(f"Data serialization failed: {str(e)}")

                # Make request with connection pooling
                response = self.session.request(method, url, **kwargs)

                # Enhanced response logging
                logger.debug(f"Response: {response.status_code} {response.reason} (Content-Length: {response.headers.get('Content-Length', 'unknown')})")

                # Handle authentication errors (don't retry)
                if response.status_code == 401:
                    error_msg = "Authentication failed with internal API"
                    response_text = self._safe_get_response_text(response)
                    logger.error(f"{error_msg}: {response_text}")
                    raise AuthenticationError(f"{error_msg}: {response_text}")

                # Handle client errors (don't retry most 4xx)
                if 400 <= response.status_code < 500 and response.status_code not in {408, 429}:
                    error_msg = f"Client error: {response.status_code} {response.reason}"
                    response_text = self._safe_get_response_text(response)
                    logger.error(f"{error_msg}: {response_text}")
                    raise APIRequestError(f"{error_msg}: {response_text}")

                # Handle server errors (retry these)
                if response.status_code in retry_statuses:
                    error_msg = f"Server error: {response.status_code} {response.reason}"
                    response_text = self._safe_get_response_text(response)

                    if attempt < max_retries:
                        sleep_time = backoff_factor * (2 ** attempt)
                        logger.warning(f"{error_msg} - retrying in {sleep_time:.1f}s (attempt {attempt + 1}/{max_retries + 1})")
                        time.sleep(sleep_time)
                        continue
                    else:
                        logger.error(f"{error_msg} - max retries exceeded: {response_text}")
                        raise APIRequestError(f"{error_msg}: {response_text}")

                # Handle rate limiting (429)
                if response.status_code == 429:
                    retry_after = int(response.headers.get('Retry-After', backoff_factor * (2 ** attempt)))
                    if attempt < max_retries:
                        logger.warning(f"Rate limited - retrying in {retry_after}s (attempt {attempt + 1}/{max_retries + 1})")
                        time.sleep(retry_after)
                        continue
                    else:
                        raise APIRequestError(f"Rate limited after {max_retries + 1} attempts")

                # Success case
                if response.ok:
                    return self._parse_response(response, endpoint)

                # Other errors
                error_msg = f"Request failed: {response.status_code} {response.reason}"
                response_text = self._safe_get_response_text(response)
                logger.error(f"{error_msg}: {response_text}")
                raise APIRequestError(f"{error_msg}: {response_text}")

            except (requests.exceptions.Timeout, requests.exceptions.ConnectionError) as e:
                last_exception = e
                error_type = "timeout" if isinstance(e, requests.exceptions.Timeout) else "connection"

                if attempt < max_retries:
                    sleep_time = backoff_factor * (2 ** attempt)
                    logger.warning(f"Request {error_type} error - retrying in {sleep_time:.1f}s (attempt {attempt + 1}/{max_retries + 1}): {str(e)}")
                    time.sleep(sleep_time)
                    continue
                else:
                    logger.error(f"Request {error_type} error after {max_retries + 1} attempts: {str(e)}")
                    raise APIRequestError(f"Request {error_type} error: {str(e)}")

            except requests.exceptions.RequestException as e:
                last_exception = e
                error_msg = f"Request exception: {str(e)}"
                logger.error(error_msg)
                raise APIRequestError(error_msg)

            except (AuthenticationError, APIRequestError):
                # Re-raise these without retrying
                raise

            except Exception as e:
                last_exception = e
                error_msg = f"Unexpected error during API request: {str(e)}"
                logger.error(error_msg, exc_info=True)
                raise APIRequestError(error_msg)

        # This shouldn't be reached, but just in case
        error_msg = f"Request failed after {max_retries + 1} attempts"
        if last_exception:
            error_msg += f": {str(last_exception)}"
        raise APIRequestError(error_msg)

    def _safe_get_response_text(self, response: requests.Response, max_length: int = 500) -> str:
        """Safely get response text with error handling and length limiting."""
        try:
            text = response.text
            if len(text) > max_length:
                return f"{text[:max_length]}... (truncated)"
            return text
        except Exception as e:
            return f"<Could not read response text: {str(e)}>"

    def _parse_response(self, response: requests.Response, endpoint: str) -> Dict[str, Any]:
        """Enhanced response parsing with better error handling."""
        try:
            # Check content type
            content_type = response.headers.get('Content-Type', '').lower()

            if 'application/json' in content_type:
                json_data = response.json()
                logger.debug(f"Successfully parsed JSON response from {endpoint}")
                return json_data
            elif response.text.strip():
                # Try to parse as JSON anyway (some APIs don't set correct Content-Type)
                try:
                    json_data = response.json()
                    logger.debug(f"Successfully parsed JSON response (incorrect Content-Type) from {endpoint}")
                    return json_data
                except json.JSONDecodeError:
                    # Return raw text
                    logger.debug(f"Returning raw text response from {endpoint}")
                    return {'raw_response': response.text}
            else:
                # Empty response
                logger.debug(f"Empty response from {endpoint}")
                return {}

        except json.JSONDecodeError as e:
            logger.warning(f"Failed to parse JSON response from {endpoint}: {str(e)}")
            return {'raw_response': response.text, 'parse_error': str(e)}
        except Exception as e:
            logger.error(f"Unexpected error parsing response from {endpoint}: {str(e)}")
            return {'error': f"Response parsing failed: {str(e)}"}

    def get(self, endpoint: str, params: Optional[Dict[str, Any]] = None, organization_id: Optional[str] = None) -> Dict[str, Any]:
        """Make GET request."""
        return self._make_request('GET', endpoint, params=params, organization_id=organization_id)

    def post(self, endpoint: str, data: Dict[str, Any], organization_id: Optional[str] = None) -> Dict[str, Any]:
        """Make POST request."""
        return self._make_request('POST', endpoint, data=data, organization_id=organization_id)

    def put(self, endpoint: str, data: Dict[str, Any], organization_id: Optional[str] = None) -> Dict[str, Any]:
        """Make PUT request."""
        return self._make_request('PUT', endpoint, data=data, organization_id=organization_id)

    def patch(self, endpoint: str, data: Dict[str, Any], organization_id: Optional[str] = None) -> Dict[str, Any]:
        """Make PATCH request."""
        return self._make_request('PATCH', endpoint, data=data, organization_id=organization_id)

    def delete(self, endpoint: str, organization_id: Optional[str] = None) -> Dict[str, Any]:
        """Make DELETE request."""
        return self._make_request('DELETE', endpoint, organization_id=organization_id)

    def health_check(self) -> Dict[str, Any]:
        """Check API health status."""
        return self.get(self._build_url('health'))

    # Workflow Execution API methods
    def get_workflow_execution(self, execution_id: Union[str, uuid.UUID], organization_id: Optional[str] = None) -> Dict[str, Any]:
        """Get workflow execution with context."""
        return self.get(self._build_url('workflow_execution', f'{str(execution_id)}/'), organization_id=organization_id)

    def get_workflow_definition(self, workflow_id: Union[str, uuid.UUID], organization_id: Optional[str] = None) -> Dict[str, Any]:
        """Get workflow definition including workflow_type."""
        try:
            # Use the workflow management internal API to get workflow details
            endpoint = f'workflow-manager/workflow/{str(workflow_id)}/'
            response = self.get(endpoint, organization_id=organization_id)
            logger.info(f"Retrieved workflow definition for {workflow_id}: {response.get('workflow_type', 'unknown')}")
            return response
        except Exception as e:
            logger.error(f"Failed to get workflow definition for {workflow_id}: {str(e)}")
            # Return default structure if API call fails
            return {
                'id': str(workflow_id),
                'workflow_type': 'ETL',  # Default to ETL
                'workflow_name': 'Unknown',
                'error': str(e)
            }

    def get_pipeline_type(self, pipeline_id: Union[str, uuid.UUID], organization_id: Optional[str] = None) -> Dict[str, Any]:
        """Get pipeline type by checking APIDeployment and Pipeline models."""
        try:
            # Use the internal API endpoint for pipeline type resolution
            endpoint = f'workflow-manager/pipeline-type/{str(pipeline_id)}/'
            response = self.get(endpoint, organization_id=organization_id)

            pipeline_type = response.get('pipeline_type', 'ETL')
            source = response.get('source', 'unknown')

            logger.debug(f"Retrieved pipeline type for {pipeline_id}: {pipeline_type} (from {source})")
            return response

        except Exception as e:
            # This is expected for non-API deployments - pipeline endpoint doesn't exist
            logger.debug(f"Pipeline type API not available for {pipeline_id} (expected for ETL workflows): {str(e)}")
            # Return default structure - this is normal behavior
            return {
                'pipeline_id': str(pipeline_id),
                'pipeline_type': 'ETL',  # Default to ETL for non-API workflows
                'source': 'fallback',
                'note': 'Pipeline type API not available - defaulted to ETL'
            }

    def update_workflow_execution_status(
        self,
        execution_id: Union[str, uuid.UUID],
        status: str,
        error_message: Optional[str] = None,
        total_files: Optional[int] = None,
        attempts: Optional[int] = None,
        execution_time: Optional[float] = None,
        organization_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """Update workflow execution status."""
        data = {'status': status}

        if error_message is not None:
            data['error_message'] = error_message
        if total_files is not None:
            data['total_files'] = total_files
        if attempts is not None:
            data['attempts'] = attempts
        if execution_time is not None:
            data['execution_time'] = execution_time

        return self.post(self._build_url('workflow_execution', f'{str(execution_id)}/update_status/'), data, organization_id=organization_id)

    def create_file_batch(
        self,
        workflow_execution_id: Union[str, uuid.UUID],
        files: list,
        is_api: bool = False,
        organization_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """Create file execution batch."""
        data = {
            'workflow_execution_id': str(workflow_execution_id),  # Ensure UUID is converted to string
            'files': files,
            'is_api': is_api
        }
        return self.post('workflow-manager/file-batch/', data, organization_id=organization_id)

    # Webhook API methods
    def send_webhook(
        self,
        url: str,
        payload: Dict[str, Any],
        notification_id: Optional[str] = None,
        authorization_type: str = 'none',
        authorization_key: Optional[str] = None,
        authorization_header: Optional[str] = None,
        timeout: int = 30,
        max_retries: int = 3,
        retry_delay: int = 5,
        headers: Optional[Dict[str, str]] = None
    ) -> Dict[str, Any]:
        """Send webhook notification."""
        data = {
            'url': url,
            'payload': payload,
            'authorization_type': authorization_type,
            'timeout': timeout,
            'max_retries': max_retries,
            'retry_delay': retry_delay
        }

        if notification_id:
            data['notification_id'] = notification_id
        if authorization_key:
            data['authorization_key'] = authorization_key
        if authorization_header:
            data['authorization_header'] = authorization_header
        if headers:
            data['headers'] = headers

        return self.post(self._build_url('webhook', 'send/'), data)

    def get_webhook_status(self, task_id: str) -> Dict[str, Any]:
        """Get webhook delivery status."""
        return self.get(self._build_url('webhook', f'status/{task_id}/'))

    def test_webhook(
        self,
        url: str,
        payload: Dict[str, Any],
        authorization_type: str = 'none',
        authorization_key: Optional[str] = None,
        timeout: int = 10
    ) -> Dict[str, Any]:
        """Test webhook configuration."""
        data = {
            'url': url,
            'payload': payload,
            'authorization_type': authorization_type,
            'timeout': timeout
        }

        if authorization_key:
            data['authorization_key'] = authorization_key

        return self.post(self._build_url('webhook', 'test/'), data)

    # Organization API methods
    def get_organization_context(self, org_id: str) -> Dict[str, Any]:
        """Get organization context."""
        return self.get(self._build_url('organization', f'{org_id}/context/'))

    def set_organization_context(self, org_id: str):
        """Set organization context for subsequent requests."""
        self.organization_id = org_id
        self.session.headers['X-Organization-ID'] = org_id
        logger.debug(f"Set organization context to {org_id}")

    def clear_organization_context(self):
        """Clear organization context."""
        self.organization_id = None
        if 'X-Organization-ID' in self.session.headers:
            del self.session.headers['X-Organization-ID']
        logger.debug("Cleared organization context")

    # Tool Execution API methods
    def execute_tool(
        self,
        tool_instance_id: Union[str, uuid.UUID],
        input_data: Dict[str, Any],
        file_data: Optional[Dict[str, Any]] = None,
        execution_context: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """Execute a tool instance with provided data."""
        logger.info(f"***********Executing tool {tool_instance_id} with input data: {input_data}")
        data = {
            'input_data': input_data,
            'file_data': file_data or {},
            'execution_context': execution_context or {}
        }
        return self.post(self._build_url('tool_execution', f'{str(tool_instance_id)}/execute/'), data)

    def get_tool_execution_status(self, execution_id: Union[str, uuid.UUID]) -> Dict[str, Any]:
        """Get tool execution status."""
        return self.get(self._build_url('tool_execution', f'status/{str(execution_id)}/'))

    def get_tool_instances_by_workflow(self, workflow_id: Union[str, uuid.UUID]) -> Dict[str, Any]:
        """Get tool instances for a workflow."""
        return self.get(self._build_url('tool_execution', f'workflow/{str(workflow_id)}/instances/'))

    def get_tool_by_id(self, tool_id: Union[str, uuid.UUID]) -> Dict[str, Any]:
        """Get tool information by tool ID."""
        return self.get(self._build_url('tool_execution', f'tool/{str(tool_id)}/'))

    # File History API methods
    def get_file_history_by_cache_key(
        self,
        cache_key: str,
        workflow_id: Union[str, uuid.UUID],
        file_path: Optional[str] = None
    ) -> Dict[str, Any]:
        """Get file history by cache key."""
        params = {'workflow_id': str(workflow_id)}
        if file_path:
            params['file_path'] = file_path
        return self.get(self._build_url('file_history', f'cache-key/{cache_key}/'), params=params)

    def create_file_history(
        self,
        workflow_id: Union[str, uuid.UUID],
        file_name: str,
        file_path: str,
        result: Optional[str] = None,
        metadata: Optional[str] = None,
        status: str = 'COMPLETED',
        error: Optional[str] = None,
        provider_file_uuid: Optional[str] = None,
        is_api: bool = False,
        file_size: int = 0,
        file_hash: str = '',
        mime_type: str = ''
    ) -> Dict[str, Any]:
        """Create file history record matching backend expected format."""
        # Build file_hash object as expected by backend
        file_hash_data = {
            'file_name': file_name,
            'file_path': file_path,
            'file_size': file_size,
            'file_hash': file_hash,
            'provider_file_uuid': provider_file_uuid,
            'mime_type': mime_type,
            'fs_metadata': {}
        }

        # Remove None values
        file_hash_data = {k: v for k, v in file_hash_data.items() if v is not None}

        data = {
            'workflow_id': str(workflow_id),
            'file_hash': file_hash_data,
            'is_api': is_api,
            'status': status
        }

        # Add optional fields if provided
        if result is not None:
            data['result'] = result
        if metadata is not None:
            data['metadata'] = metadata
        if error is not None:
            data['error'] = error

        logger.info(f"Creating file history record for {file_name} with status: {status}")
        logger.debug(f"File history data: {data}")

        try:
            response = self.post(self._build_url('file_history', 'create/'), data)
            logger.info(f"Successfully created file history record: {response.get('id') if response else 'unknown'}")
            return response
        except Exception as e:
            logger.error(f"Failed to create file history record: {str(e)}")
            logger.debug(f"Request data was: {data}")
            raise

    def get_file_history_status(self, file_history_id: Union[str, uuid.UUID]) -> Dict[str, Any]:
        """Get file history status."""
        return self.get(self._build_url('file_history', f'status/{str(file_history_id)}/'))

    def get_file_history(
        self,
        workflow_id: Union[str, uuid.UUID],
        provider_file_uuid: str,
        file_path: str,
        organization_id: str
    ) -> Dict[str, Any]:
        """
        Get file history using backend FileHistoryHelper for consistent deduplication.

        This method calls the same FileHistoryHelper.get_file_history() logic
        used by the backend source.py to ensure consistent deduplication behavior.

        Args:
            workflow_id: Workflow ID
            provider_file_uuid: Provider file UUID (from source connector)
            file_path: File path
            organization_id: Organization ID

        Returns:
            Dictionary with file history data or None if not found
        """
        data = {
            'workflow_id': str(workflow_id),
            'provider_file_uuid': provider_file_uuid,
            'file_path': file_path,
            'organization_id': organization_id
        }

        try:
            logger.debug(f"Getting file history for workflow {workflow_id}, provider_uuid: {provider_file_uuid}, file_path: {file_path}")
            response = self.post(self._build_url('file_history', 'get/'), data)
            logger.debug(f"File history response: found={response.get('found')}")
            return response
        except Exception as e:
            logger.error(f"Failed to get file history: {str(e)}")
            logger.debug(f"Request data was: {data}")
            raise

    # Execution Finalization API methods
    @circuit_breaker(failure_threshold=3, recovery_timeout=120.0)
    def finalize_workflow_execution(
        self,
        execution_id: Union[str, uuid.UUID],
        final_status: str = 'COMPLETED',
        total_files_processed: Optional[int] = None,  # No default - only include if provided
        total_execution_time: float = 0.0,
        results_summary: Optional[Dict[str, Any]] = None,
        error_summary: Optional[Dict[str, Any]] = None,
        organization_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """Finalize workflow execution with organization context.

        Args:
            execution_id: Execution ID to finalize
            final_status: Final execution status
            total_files_processed: Number of files processed
            total_execution_time: Total execution time in seconds
            results_summary: Summary of results
            error_summary: Summary of errors
            organization_id: Organization ID for context (optional)
        """
        data = {
            'final_status': final_status,
            'total_execution_time': total_execution_time,
            'results_summary': results_summary or {},
            'error_summary': error_summary or {}
        }

        # Only include total_files_processed if explicitly provided and positive
        if total_files_processed is not None and total_files_processed > 0:
            data['total_files_processed'] = total_files_processed

        # Include organization context if provided
        if organization_id:
            data['organization_id'] = organization_id
            logger.debug(f"Including organization_id {organization_id} in finalization request")

        try:
            response = self.post(
                self._build_url('execution', f'finalize/{str(execution_id)}/'),
                data,
                organization_id=organization_id
            )
            logger.debug(f"Finalization API call successful for execution {execution_id}")
            return response
        except CircuitBreakerOpenError:
            logger.warning(f"Finalization endpoint circuit breaker open - using fallback status update")
            # Circuit breaker is open, use fallback immediately
            return self.update_workflow_execution_status(
                execution_id=str(execution_id),
                status=final_status
            )
        except Exception as e:
            logger.error(f"Finalization API call failed for execution {execution_id}: {str(e)}")
            # Check for specific error patterns that require fallback
            error_str = str(e)
            if any(pattern in error_str for pattern in [
                "'WorkflowExecution' object has no attribute 'organization_id'",
                "too many error responses",
                "500 Internal Server Error",
                "503 Service Unavailable",
                "502 Bad Gateway"
            ]):
                logger.warning(f"Known backend error detected - using fallback status update: {error_str[:100]}...")
                # Fallback to simple status update that avoids the problematic backend code path
                return self.update_workflow_execution_status(
                    execution_id=str(execution_id),
                    status=final_status
                )
            else:
                raise e

    @circuit_breaker(failure_threshold=2, recovery_timeout=30.0)  # More lenient for cleanup
    def cleanup_execution_resources(
        self,
        execution_ids: List[Union[str, uuid.UUID]],
        cleanup_types: Optional[list] = None
    ) -> Dict[str, Any]:
        """Cleanup execution resources."""
        data = {
            'execution_ids': [str(eid) for eid in execution_ids],  # Convert all IDs to strings
            'cleanup_types': cleanup_types or ['cache', 'temp_files']
        }
        return self.post(self._build_url('execution', 'cleanup/'), data)

    def get_execution_finalization_status(self, execution_id: Union[str, uuid.UUID]) -> Dict[str, Any]:
        """Get execution finalization status."""
        return self.get(self._build_url('execution', f'finalization-status/{str(execution_id)}/'))

    # REMOVED: execute_workflow_for_file
    # This method has been moved to workers/shared/workflow_orchestrator.py
    # to implement worker-native workflow execution without backend dependency.
    # Use WorkerWorkflowOrchestrator.execute_workflow_file() instead.

    def get_or_create_workflow_file_execution1(
        self,
        execution_id: Union[str, UUID],
        file_hash: Dict[str, Any],
        workflow_id: Union[str, UUID],
        organization_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """Get or create a workflow file execution record."""
        data = {
            'execution_id': str(execution_id),
            'file_hash': file_hash,
            'workflow_id': str(workflow_id)
        }
        logger.info(f"Workflow file execution data: {data}")
        logger.info(f"Workflow file execution URL: {self._build_url('file_execution')}")
        return self.post(self._build_url('file_execution'), data, organization_id=organization_id)

    def get_or_create_workflow_file_execution(
        self,
        execution_id: Union[str, UUID],
        file_hash: Union[Dict[str, Any], FileHashData],
        workflow_id: Union[str, UUID],
        organization_id: Optional[str] = None
    ) -> WorkflowFileExecutionData:
        """Get or create a workflow file execution record using shared dataclasses."""
        # Convert file_hash to FileHashData if it's a dict
        if isinstance(file_hash, dict):
            file_hash_data = FileHashData.from_dict(file_hash)
        else:
            file_hash_data = file_hash

        # Debug logging to understand API file detection
        logger.info(f"FileHashData debug: file_name='{file_hash_data.file_name}', "
                   f"has_hash={file_hash_data.has_hash()}, "
                   f"source_connection_type='{getattr(file_hash_data, 'source_connection_type', None)}'")

        # CRITICAL FIX: For API files with pre-calculated hash, skip hash computation
        # to prevent temporary MD5 path hash generation and validation errors
        if file_hash_data.has_hash() and getattr(file_hash_data, 'source_connection_type', None) == 'API':
            logger.info(f"API file with pre-calculated hash: {file_hash_data.file_hash[:16]}... - skipping validation")
        else:
            logger.info(f"Non-API file or file without hash: {file_hash_data.file_name} - performing validation")

            # For non-API files or files without hash, validate what we have
            try:
                file_hash_data.validate_for_api()
                logger.debug(f"FileHashData validation successful for {file_hash_data.file_name}")
            except ValueError as validation_error:
                logger.error(f"FileHashData validation failed: {str(validation_error)}")
                logger.error(f"FileHashData details: file_name='{file_hash_data.file_name}', "
                            f"file_path='{file_hash_data.file_path}', "
                            f"file_hash='{file_hash_data.file_hash[:16] if file_hash_data.file_hash else 'None'}...', "
                            f"provider_file_uuid='{file_hash_data.provider_file_uuid}'")

                # Provide actionable error message based on validation failure
                if not file_hash_data.file_name:
                    raise APIRequestError("File name is required for WorkflowFileExecution creation")
                elif not file_hash_data.file_path:
                    raise APIRequestError("File path is required for WorkflowFileExecution creation")
                else:
                    raise APIRequestError(f"FileHashData validation failed: {str(validation_error)}. "
                                        f"Check file path accessibility and connector configuration.") from validation_error

        # First try to get existing record - CRITICAL FIX: Match backend manager logic
        params = {
            'execution_id': str(execution_id),
            'workflow_id': str(workflow_id),
            'file_path': file_hash_data.file_path  # CRITICAL: Include file_path to match unique constraints
        }

        # Match backend manager logic: use file_hash OR provider_file_uuid (not both)
        if file_hash_data.file_hash:
            params['file_hash'] = file_hash_data.file_hash
            logger.debug(f"Using file_hash for lookup: {file_hash_data.file_hash}")
        elif file_hash_data.provider_file_uuid:
            params['provider_file_uuid'] = file_hash_data.provider_file_uuid
            logger.debug(f"Using provider_file_uuid for lookup: {file_hash_data.provider_file_uuid}")
        else:
            logger.warning(f"No file_hash or provider_file_uuid available for lookup - this may cause issues")

        logger.debug(f"Lookup parameters: {params}")

        try:
            # Try to get existing record
            response = self.get(self._build_url('file_execution'), params=params, organization_id=organization_id)
            if response and isinstance(response, list) and len(response) > 0:
                logger.debug(f"Found existing workflow file execution: {response[0].get('id')}")
                return WorkflowFileExecutionData.from_dict(response[0])
        except Exception as e:
            logger.debug(f"Could not get existing workflow file execution: {str(e)}")
            pass  # Continue to create if not found

        # Create request using shared dataclass
        create_request = FileExecutionCreateRequest(
            execution_id=execution_id,
            file_hash=file_hash_data,
            workflow_id=workflow_id
        )

        data = create_request.to_dict()

        logger.info(f"Creating workflow file execution with file_hash: {file_hash_data.file_name}")
        logger.info(f"FileHashData key identifiers: provider_file_uuid='{file_hash_data.provider_file_uuid}', file_path='{file_hash_data.file_path}', file_hash='{file_hash_data.file_hash}'")
        logger.debug(f"FileHashData: {file_hash_data.to_dict()}")

        try:
            result = self.post(self._build_url('file_execution'), data, organization_id=organization_id)

            # Handle both list and dict responses (defensive programming)
            if isinstance(result, list):
                logger.warning(f"Backend returned list instead of dict for file execution create: {len(result)} items")
                if len(result) > 0:
                    file_execution_dict = result[0]  # Take first item from list
                    logger.info(f"Successfully created workflow file execution: {file_execution_dict.get('id') if isinstance(file_execution_dict, dict) else 'unknown'}")
                    return WorkflowFileExecutionData.from_dict(file_execution_dict)
                else:
                    logger.error("Backend returned empty list for file execution create")
                    raise APIRequestError("Backend returned empty list for file execution create")
            elif isinstance(result, dict):
                file_execution_id = result.get('id', 'unknown')
                logger.info(f"Successfully created workflow file execution: {file_execution_id}")
                logger.info(f"Created for file: {file_hash_data.file_name} with provider_file_uuid: {file_hash_data.provider_file_uuid}")
                return WorkflowFileExecutionData.from_dict(result)
            else:
                logger.error(f"Backend returned unexpected type for file execution create: {type(result)}")
                raise APIRequestError(f"Backend returned unexpected response type: {type(result)}")
        except Exception as e:
            logger.error(f"Failed to create workflow file execution: {str(e)}", exc_info=True)
            logger.debug(f"Request data was: {data}")
            raise

    def update_workflow_file_execution_hash(
        self,
        file_execution_id: Union[str, UUID],
        file_hash: str,
        fs_metadata: Optional[Dict[str, Any]] = None,
        organization_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """Update workflow file execution with computed file hash.

        This method should be used when the SHA256 content hash is computed
        after the WorkflowFileExecution record is initially created.

        Args:
            file_execution_id: ID of the WorkflowFileExecution record to update
            file_hash: Computed SHA256 hash of file content
            fs_metadata: Optional filesystem metadata to update
            organization_id: Organization ID for context

        Returns:
            Updated WorkflowFileExecution data
        """
        data = {'file_hash': file_hash}
        if fs_metadata:
            data['fs_metadata'] = fs_metadata

        logger.info(f"Updating file execution {file_execution_id} with computed hash: {file_hash[:16]}...")

        try:
            response = self.patch(
                self._build_url('file_execution', f'{str(file_execution_id)}/update_hash/'),
                data,
                organization_id=organization_id
            )

            logger.info(f"Successfully updated file execution {file_execution_id} with hash")
            return response

        except Exception as e:
            logger.error(f"Failed to update file execution hash: {str(e)}")
            raise APIRequestError(f"Failed to update file execution hash: {str(e)}") from e

    def update_file_execution_status(
        self,
        file_execution_id: Union[str, UUID],
        status: str,
        execution_time: Optional[float] = None,
        error_message: Optional[str] = None,
        organization_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """Update workflow file execution status with execution time.

        This method updates the WorkflowFileExecution record with status and execution time,
        matching the Django model's update_status() method behavior.

        Args:
            file_execution_id: ID of the WorkflowFileExecution record to update
            status: New execution status (COMPLETED, ERROR, STOPPED, etc.)
            execution_time: Execution time in seconds (optional)
            error_message: Error message if status is ERROR (optional)
            organization_id: Organization ID for context

        Returns:
            Updated WorkflowFileExecution data
        """
        data = {'status': status}

        if execution_time is not None:
            data['execution_time'] = execution_time
        if error_message is not None:
            data['execution_error'] = error_message

        logger.info(f"Updating file execution {file_execution_id} status to {status}"
                   f"{f' with execution_time {execution_time:.2f}s' if execution_time else ''}")

        try:
            response = self.post(
                self._build_url('file_execution', f'{str(file_execution_id)}/status/'),
                data,
                organization_id=organization_id
            )

            logger.info(f"Successfully updated file execution {file_execution_id} status")
            return response

        except Exception as e:
            logger.error(f"Failed to update file execution status: {str(e)}")
            raise APIRequestError(f"Failed to update file execution status: {str(e)}") from e

    def get_workflow_endpoints(self, workflow_id: Union[str, UUID], organization_id: Optional[str] = None) -> Dict[str, Any]:
        """Get workflow endpoints for a specific workflow.

        Args:
            workflow_id: Workflow ID to get endpoints for
            organization_id: Organization ID for context (optional)

        Returns:
            Dictionary with endpoint data including 'endpoints' list and 'has_api_endpoints' flag
        """
        # Use the workflow-manager endpoint pattern: /workflow-manager/<workflow_id>/endpoint/
        endpoint = f"workflow-manager/{workflow_id}/endpoint/"

        try:
            response = self._make_request(
                method='GET',
                endpoint=endpoint,
                timeout=self.config.api_timeout,
                organization_id=organization_id
            )

            # The API returns a dict with 'endpoints', 'has_api_endpoints', etc.
            if isinstance(response, dict):
                endpoint_count = len(response.get('endpoints', []))
                logger.debug(f"Retrieved {endpoint_count} endpoints for workflow {workflow_id}")
                return response
            else:
                logger.warning(f"Unexpected response format for workflow endpoints: {type(response)}")
                return {'endpoints': [], 'has_api_endpoints': False, 'total_endpoints': 0}

        except Exception as e:
            logger.error(f"Failed to get workflow endpoints for {workflow_id}: {str(e)}")
            return {'endpoints': [], 'has_api_endpoints': False, 'total_endpoints': 0}

    @circuit_breaker(failure_threshold=3, recovery_timeout=60.0)
    def check_file_history_batch(
        self,
        workflow_id: Union[str, UUID],
        file_hashes: List[str],
        organization_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Check file history for a batch of file hashes.

        Args:
            workflow_id: Workflow ID to check file history for
            file_hashes: List of file hashes to check
            organization_id: Organization ID for context (optional)

        Returns:
            Dictionary with 'processed_file_hashes' list
        """
        endpoint = "workflow-manager/file-history/batch-check/"

        payload = {
            'workflow_id': str(workflow_id),
            'file_hashes': file_hashes,
            'organization_id': organization_id or self.organization_id
        }

        try:
            response = self._make_request(
                method='POST',
                endpoint=endpoint,
                data=payload,
                timeout=self.config.api_timeout,
                organization_id=organization_id
            )

            logger.debug(f"File history batch check for {len(file_hashes)} hashes: {len(response.get('processed_file_hashes', []))} already processed")
            return response

        except Exception as e:
            logger.error(f"Failed to check file history batch for workflow {workflow_id}: {str(e)}")
            # Return empty result to continue without file history filtering
            return {'processed_file_hashes': []}

    @circuit_breaker(failure_threshold=3, recovery_timeout=60.0)
    def create_file_history_entry(self, history_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Create a file history entry via internal API.

        Args:
            history_data: Dictionary containing file history data

        Returns:
            Creation result dictionary
        """
        endpoint = "workflow-manager/file-history/create/"

        try:
            response = self._make_request(
                method='POST',
                endpoint=endpoint,
                data=history_data,
                timeout=self.config.api_timeout
            )

            logger.debug(f"Created file history entry for file {history_data.get('file_name', 'unknown')}")
            return response

        except Exception as e:
            logger.error(f"Failed to create file history entry: {str(e)}")
            # Return empty result to continue without breaking the flow
            return {'created': False, 'error': str(e)}

    # REMOVED: get_workflow_source_files
    # This method has been moved to workers/shared/source_operations.py
    # to implement worker-native source file listing without backend dependency.
    # Use WorkerSourceOperations.list_workflow_source_files() instead.

    @circuit_breaker(failure_threshold=3, recovery_timeout=60.0)
    def update_pipeline_status(
        self,
        pipeline_id: Union[str, UUID],
        execution_id: Union[str, UUID],
        status: str,
        organization_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """Update pipeline status.

        Args:
            pipeline_id: Pipeline ID
            execution_id: Execution ID
            status: New status
            organization_id: Organization ID for context (optional)

        Returns:
            Update response
        """
        data = {
            'execution_id': str(execution_id),
            'status': status
        }

        try:
            # Use the workflow-manager pipeline endpoint for status updates
            endpoint = f"workflow-manager/pipeline/{pipeline_id}/status/"

            response = self._make_request(
                method='POST',
                endpoint=endpoint,
                data=data,
                timeout=self.config.api_timeout,
                organization_id=organization_id
            )

            logger.debug(f"Updated pipeline {pipeline_id} status to {status}")
            return response

        except Exception as e:
            logger.error(f"Failed to update pipeline {pipeline_id} status: {str(e)}")
            return {'success': False, 'error': str(e)}

    # REMOVED: send_webhook_batch
    # This method has been moved to workers/shared/webhook_service.py
    # to implement worker-native webhook operations without backend dependency.
    # Use WorkerWebhookService.send_webhooks_batch() instead.

    def increment_completed_files(self, workflow_id: str, execution_id: str) -> Dict[str, Any]:
        """
        Increment completed files count for execution (like Django ExecutionCacheUtils).

        Args:
            workflow_id: Workflow ID
            execution_id: Execution ID

        Returns:
            Response from increment operation
        """
        data = {
            'workflow_id': workflow_id,
            'execution_id': execution_id,
            'increment_type': 'completed'
        }
        try:
            response = self.post('workflow-manager/increment-files/', data)
            logger.debug(f"Incremented completed files for execution {execution_id}")
            return response
        except Exception as e:
            logger.error(f"Failed to increment completed files: {str(e)}")
            return {'success': False, 'error': str(e)}

    def increment_failed_files(self, workflow_id: str, execution_id: str) -> Dict[str, Any]:
        """
        Increment failed files count for execution (like Django ExecutionCacheUtils).

        Args:
            workflow_id: Workflow ID
            execution_id: Execution ID

        Returns:
            Response from increment operation
        """
        data = {
            'workflow_id': workflow_id,
            'execution_id': execution_id,
            'increment_type': 'failed'
        }
        try:
            response = self.post('workflow-manager/increment-files/', data)
            logger.debug(f"Incremented failed files for execution {execution_id}")
            return response
        except Exception as e:
            logger.error(f"Failed to increment failed files: {str(e)}")
            return {'success': False, 'error': str(e)}

    def get_workflow_destination_config(self, workflow_id: str, execution_id: str) -> Dict[str, Any]:
        """
        Get destination configuration for workflow execution.

        Args:
            workflow_id: Workflow ID
            execution_id: Execution ID

        Returns:
            Destination configuration
        """
        try:
            # Use workflow execution endpoint to get destination config
            response = self.get(f'workflow-manager/{execution_id}/')
            # Extract destination_config from the response
            if isinstance(response, dict) and 'destination_config' in response:
                logger.debug(f"Retrieved destination config for workflow {workflow_id}")
                return response['destination_config']
            # Fallback for backward compatibility
            logger.debug(f"Retrieved full response for workflow {workflow_id}")
            return response
        except Exception as e:
            logger.error(f"Failed to get destination config: {str(e)}")
            return {'type': 'none', 'error': str(e)}

    def create_workflow_file_execution(
        self,
        workflow_execution_id: str,
        file_name: str,
        file_path: str,
        file_hash: str,
        file_execution_id: Optional[str] = None,
        file_size: int = 0,
        mime_type: str = '',
        provider_file_uuid: Optional[str] = None,
        fs_metadata: Optional[Dict[str, Any]] = None,
        status: str = 'QUEUED'
    ) -> Dict[str, Any]:
        """
        Create WorkflowFileExecution record via internal API with complete metadata.

        Uses the existing backend endpoint that expects file_hash object format.
        Now supports all the fields needed for proper file hash generation.

        Args:
            workflow_execution_id: Workflow execution ID
            file_name: File name
            file_path: File path
            file_hash: Actual computed SHA256 file hash (not dummy)
            file_execution_id: Optional unique file execution ID
            file_size: File size in bytes
            mime_type: File MIME type
            provider_file_uuid: Provider-specific file UUID
            fs_metadata: File system metadata
            status: Initial status

        Returns:
            Created WorkflowFileExecution data
        """
        try:
            # Format data to match backend WorkflowFileExecutionAPIView expectations
            data = {
                'execution_id': workflow_execution_id,
                'workflow_id': workflow_execution_id,  # Will be extracted from execution
                'file_hash': {
                    'file_name': file_name,
                    'file_path': file_path,
                    'file_hash': file_hash,  # Actual computed hash
                    'file_size': file_size,
                    'mime_type': mime_type,
                    'provider_file_uuid': provider_file_uuid,
                    'fs_metadata': fs_metadata or {},
                    'is_executed': False
                }
            }

            # Include file_execution_id if provided
            if file_execution_id:
                data['file_execution_id'] = file_execution_id

            # Log the actual hash being used
            if file_hash:
                logger.info(f"Creating WorkflowFileExecution with actual hash {file_hash} for {file_name}")
            else:
                logger.warning(f"Creating WorkflowFileExecution with empty hash for {file_name}")

            response = self.post('workflow-manager/file-execution/', data)
            logger.info(f"Successfully created WorkflowFileExecution for {file_name} with hash {file_hash[:8] if file_hash else 'empty'}...")
            return response
        except Exception as e:
            logger.error(f"Failed to create WorkflowFileExecution: {str(e)}")
            return {'success': False, 'error': str(e)}

    def update_workflow_file_execution_status(
        self,
        file_execution_id: str,
        status: str,
        result: Optional[str] = None,
        error_message: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Update WorkflowFileExecution status via internal API using shared dataclasses.

        Args:
            file_execution_id: File execution ID
            status: New status
            result: Execution result (optional)
            error_message: Error message if any

        Returns:
            Update result
        """
        try:
            # Create status update request using shared dataclass
            update_request = FileExecutionStatusUpdateRequest(
                status=status,
                error_message=error_message,
                result=result
            )

            # Use the internal file execution status endpoint
            endpoint = self._build_url('file_execution', f'{file_execution_id}/status/')
            response = self.post(endpoint, update_request.to_dict())

            logger.info(f"Successfully updated WorkflowFileExecution {file_execution_id} status to {status}")
            return response

        except Exception as e:
            logger.error(f"Failed to update WorkflowFileExecution status for {file_execution_id}: {str(e)}")
            # Don't raise exception to avoid breaking worker flow
            return {'success': False, 'error': str(e)}



    def close(self):
        """Close the HTTP session."""
        self.session.close()
        logger.debug("Closed API client session")

    def __enter__(self):
        """Context manager entry."""
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        """Context manager exit."""
        self.close()
