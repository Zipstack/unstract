"""LLM Response Cache implementation for caching LLM API responses.

This module provides an in-memory cache with TTL (Time-To-Live) support
for storing and retrieving LLM responses based on prompt and reference data.
"""

import hashlib
import time


class LLMResponseCache:
    """In-memory cache for LLM responses with TTL expiration.

    This cache is used to store LLM API responses to avoid redundant
    API calls for identical prompts and reference data combinations.
    Uses SHA256 hashing for cache key generation and supports TTL-based
    expiration for automatic cleanup.
    """

    def __init__(self, ttl_hours: int = 24):
        """Initialize the LLM response cache.

        Args:
            ttl_hours: Time-to-live in hours (default 24).
                      Cached entries expire after this duration.
        """
        self.cache: dict[str, tuple[str, float]] = {}
        self.ttl_seconds = ttl_hours * 3600

    def get(self, key: str) -> str | None:
        """Get cached response if not expired.

        Performs lazy cleanup by removing expired entries when accessed.

        Args:
            key: Cache key (generated by generate_cache_key)

        Returns:
            Cached response string if valid and not expired, None otherwise

        Example:
            >>> cache = LLMResponseCache(ttl_hours=1)
            >>> cache.set("key123", "response")
            >>> cache.get("key123")
            'response'
        """
        if key not in self.cache:
            return None

        response, expiry = self.cache[key]
        current_time = time.time()

        if current_time >= expiry:
            # Entry has expired, remove it (lazy cleanup)
            del self.cache[key]
            return None

        return response

    def set(self, key: str, response: str) -> None:
        """Cache response with TTL.

        Args:
            key: Cache key (generated by generate_cache_key)
            response: LLM response to cache

        Example:
            >>> cache = LLMResponseCache()
            >>> key = cache.generate_cache_key("prompt", "ref_data")
            >>> cache.set(key, "LLM response")
        """
        expiry = time.time() + self.ttl_seconds
        self.cache[key] = (response, expiry)

    def invalidate(self, key: str) -> bool:
        """Remove specific key from cache.

        Args:
            key: Cache key to invalidate

        Returns:
            True if key was removed, False if key didn't exist

        Example:
            >>> cache = LLMResponseCache()
            >>> cache.set("key123", "response")
            >>> cache.invalidate("key123")
            True
        """
        if key in self.cache:
            del self.cache[key]
            return True
        return False

    def invalidate_all(self) -> int:
        """Clear entire cache.

        Returns:
            Count of invalidated entries

        Example:
            >>> cache = LLMResponseCache()
            >>> cache.set("key1", "response1")
            >>> cache.set("key2", "response2")
            >>> cache.invalidate_all()
            2
        """
        count = len(self.cache)
        self.cache.clear()
        return count

    def generate_cache_key(self, prompt: str, reference_data: str) -> str:
        r"""Generate SHA256 hash from prompt + reference data.

        Creates a deterministic cache key based on the prompt and
        reference data combination. Same inputs always produce the
        same key.

        Args:
            prompt: The resolved prompt text
            reference_data: The reference data text

        Returns:
            64-character hexadecimal SHA256 hash

        Example:
            >>> cache = LLMResponseCache()
            >>> key = cache.generate_cache_key("Match vendor", "Slack\nGoogle")
            >>> len(key)
            64
        """
        combined = f"{prompt}{reference_data}"
        hash_obj = hashlib.sha256(combined.encode("utf-8"))
        return hash_obj.hexdigest()

    def get_stats(self) -> dict[str, int]:
        """Return cache statistics.

        Analyzes the current cache state and returns counts of
        total, expired, and valid entries.

        Returns:
            Dictionary with keys: 'total', 'expired', 'valid'

        Example:
            >>> cache = LLMResponseCache()
            >>> cache.set("key1", "response1")
            >>> stats = cache.get_stats()
            >>> stats["total"]
            1
        """
        current_time = time.time()
        total = len(self.cache)
        expired = sum(
            1 for _, (_, expiry) in self.cache.items() if current_time >= expiry
        )
        valid = total - expired

        return {"total": total, "expired": expired, "valid": valid}

    def cleanup_expired(self) -> int:
        """Remove expired entries.

        Performs a full scan of the cache and removes all entries
        that have exceeded their TTL. Can be called periodically
        for cache maintenance.

        Returns:
            Count of removed entries

        Example:
            >>> cache = LLMResponseCache(ttl_hours=0)  # Immediate expiry
            >>> cache.set("key1", "response1")
            >>> time.sleep(0.1)
            >>> cache.cleanup_expired()
            1
        """
        current_time = time.time()
        keys_to_remove = [
            key for key, (_, expiry) in self.cache.items() if current_time >= expiry
        ]

        for key in keys_to_remove:
            del self.cache[key]

        return len(keys_to_remove)
